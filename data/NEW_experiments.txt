=================================================================
HELLO WORLD
=================================================================

NEW_0:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0.5
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'absolute'

NEW_1:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 25,000
    epochs: 100
    hidden_sizes: 256 x 2
    controller_mode: 'absolute'

=================================================================
MEM UNITS
=================================================================

NEW_2:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 64 x 2
    controller_mode: 'absolute'

NEW_3:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 32 x 2
    controller_mode: 'absolute'

NEW_4:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 16 x 2
    controller_mode: 'absolute'

NEW_5:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 128 x 2
    controller_mode: 'absolute'

NEW_6:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 3
    controller_mode: 'absolute'

NEW_7:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'absolute'

=================================================================
INCREMENTAL CONTROLLER MODE, nreg_factor AS HPARAM
=================================================================

NEW_8:
    base_rew: 0
    rmse_factor: 2
    reg_factor: 0
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'incremental'
    action_min = -0.1
    action_max = 0.1

NEW_9:
    base_rew: 0
    rmse_factor: 2
    areg_factor: 0
    nreg_factor: 0.5
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'incremental'
    action_min = -0.1
    action_max = 0.1

NEW_10:
    base_rew: 0
    rmse_factor: 2
    areg_factor: 0
    nreg_factor: 1
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'incremental'
    action_min = -0.1
    action_max = 0.1

NEW_11:
    base_rew: 0
    rmse_factor: 2
    areg_factor: 0
    nreg_factor: 2
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'incremental'
    action_min = -0.1
    action_max = 0.1

NEW_12: <---------- RUNNING
    base_rew: 0
    rmse_factor: 2
    areg_factor: 0
    nreg_factor: 4
    start_steps: 10,000
    epochs: 50
    hidden_sizes: 256 x 2
    controller_mode: 'incremental'
    action_min = -0.1
    action_max = 0.1



also train for 1 or two touches, then plot response of that "untrained" policy.
how different from "trained"?
or just use purely random "policy"...


For test episodes: always start at same playhead? Not random? And if "done" then just keep going?